penguins <- read.csv("palmer_penguins.csv")
# View structure
str(penguins)
# Keep only bill_length_mm and flipper_length_mm, and remove missing values
penguins_clean <- na.omit(penguins[, c("bill_length_mm", "flipper_length_mm")])
kmeans_custom <- function(data, k, max_iter = 100, seed = 42){
set.seed(seed)
n <- nrow(data)
centroids <- data[sample(1:n, k), ]
cluster_history <- list(centroids)
for (i in 1:max_iter) {
dists <- as.matrix(dist(rbind(centroids, data)))[1:k, (k+1):(k+n)]
clusters <- apply(dists, 2, which.min)
new_centroids <- aggregate(data, by = list(clusters), FUN = mean)[, -1]
cluster_history[[i + 1]] <- new_centroids
if (all(abs(as.matrix(centroids) - as.matrix(new_centroids)) < 1e-6)) break
centroids <- new_centroids
}
list(clusters = clusters, centroids = centroids, history = cluster_history)
}
penguin_matrix <- as.matrix(penguins_clean)
res_custom <- kmeans_custom(penguin_matrix, k = 3)
penguins_clean$cluster_custom <- as.factor(res_custom$clusters)
library(ggplot2)
ggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_custom)) +
geom_point(size = 3) +
geom_point(data = as.data.frame(res_custom$centroids),
aes(x = bill_length_mm, y = flipper_length_mm),
color = "red", shape = 4, size = 5) +
labs(title = "Custom K-means Clustering") +
theme_minimal()
res_builtin <- kmeans(penguin_matrix, centers = 3)
penguins_clean$cluster_builtin <- as.factor(res_builtin$cluster)
ggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_builtin)) +
geom_point(size = 3) +
geom_point(data = as.data.frame(res_builtin$centers),
aes(x = bill_length_mm, y = flipper_length_mm),
color = "red", shape = 4, size = 5) +
labs(title = "Built-in K-means Clustering") +
theme_minimal()
penguins <- read.csv("palmer_penguins.csv")
# View structure
str(penguins)
penguins <- read.csv("palmer_penguins.csv")
# View structure
str(penguins)
# Keep only bill_length_mm and flipper_length_mm, and remove missing values
penguins_clean <- na.omit(penguins[, c("bill_length_mm", "flipper_length_mm")])
kmeans_custom <- function(data, k, max_iter = 100, seed = 42){
set.seed(seed)
n <- nrow(data)
centroids <- data[sample(1:n, k), ]
cluster_history <- list(centroids)
for (i in 1:max_iter) {
dists <- as.matrix(dist(rbind(centroids, data)))[1:k, (k+1):(k+n)]
clusters <- apply(dists, 2, which.min)
new_centroids <- aggregate(data, by = list(clusters), FUN = mean)[, -1]
cluster_history[[i + 1]] <- new_centroids
if (all(abs(as.matrix(centroids) - as.matrix(new_centroids)) < 1e-6)) break
centroids <- new_centroids
}
list(clusters = clusters, centroids = centroids, history = cluster_history)
}
penguin_matrix <- as.matrix(penguins_clean)
res_custom <- kmeans_custom(penguin_matrix, k = 3)
penguins_clean$cluster_custom <- as.factor(res_custom$clusters)
library(ggplot2)
ggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_custom)) +
geom_point(size = 3) +
geom_point(data = as.data.frame(res_custom$centroids),
aes(x = bill_length_mm, y = flipper_length_mm),
color = "red", shape = 4, size = 5) +
labs(title = "Custom K-means Clustering") +
theme_minimal()
res_builtin <- kmeans(penguin_matrix, centers = 3)
penguins_clean$cluster_builtin <- as.factor(res_builtin$cluster)
ggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_builtin)) +
geom_point(size = 3) +
geom_point(data = as.data.frame(res_builtin$centers),
aes(x = bill_length_mm, y = flipper_length_mm),
color = "red", shape = 4, size = 5) +
labs(title = "Built-in K-means Clustering") +
theme_minimal()
# Load required libraries
library(cluster)     # for silhouette()
library(factoextra)  # for fviz_nbclust()
install.packages("factoextra")
# Load required libraries
library(cluster)     # for silhouette()
library(factoextra)  # for fviz_nbclust()
library(ggplot2)
# Load the data
penguins <- read.csv("palmer_penguins.csv")
# Clean the data
penguins_clean <- na.omit(penguins[, c("bill_length_mm", "flipper_length_mm")])
# Function to compute total within-cluster sum of square
wcss <- numeric()
for (k in 2:7) {
km <- kmeans(penguins_clean, centers = k, nstart = 25)
wcss[k - 1] <- km$tot.withinss
}
# Plot WCSS (Elbow Method)
plot(2:7, wcss, type = "b", pch = 19, frame = FALSE,
xlab = "Number of clusters K", ylab = "Total Within-Cluster Sum of Squares (WCSS)",
main = "Elbow Method for Optimal K")
sil_scores <- numeric()
for (k in 2:7) {
km <- kmeans(penguins_clean, centers = k, nstart = 25)
sil <- silhouette(km$cluster, dist(penguins_clean))
sil_scores[k - 1] <- mean(sil[, 3])  # average silhouette width
}
# Plot Silhouette Scores
plot(2:7, sil_scores, type = "b", pch = 19, col = "forestgreen", frame = FALSE,
xlab = "Number of clusters K", ylab = "Average Silhouette Score",
main = "Silhouette Method for Optimal K")
# gen data -----
set.seed(42)
n <- 100
x1 <- runif(n, -3, 3)
x2 <- runif(n, -3, 3)
x <- cbind(x1, x2)
# define a wiggly boundary
boundary <- sin(4*x1) + x1
y <- ifelse(x2 > boundary, 1, 0) |> as.factor()
dat <- data.frame(x1 = x1, x2 = x2, y = y)
library(ggplot2)
ggplot(dat, aes(x = x1, y = x2, color = y)) +
geom_point(size = 3) +
labs(title = "Synthetic Data for KNN",
subtitle = "Wiggly Decision Boundary",
x = "x1", y = "x2") +
theme_minimal()
# Plot data points and add wiggly boundary
ggplot(dat, aes(x = x1, y = x2, color = y)) +
geom_point(size = 3) +
stat_function(fun = function(x) sin(4 * x) + x, color = "black", size = 1, linetype = "dashed") +
labs(title = "KNN Synthetic Data with Wiggly Boundary",
x = "x1",
y = "x2",
color = "Class (y)") +
theme_minimal()
# Generate test dataset -----
set.seed(999)  # different seed than training data
n_test <- 100
x1_test <- runif(n_test, -3, 3)
x2_test <- runif(n_test, -3, 3)
boundary_test <- sin(4 * x1_test) + x1_test
y_test <- ifelse(x2_test > boundary_test, 1, 0) |> as.factor()
test_dat <- data.frame(x1 = x1_test, x2 = x2_test, y = y_test)
# Load required package
library(class)
# Built-in knn()
builtin_preds <- knn(train = dat[, 1:2],
test = test_dat[, 1:2],
cl = dat$y,
k = 5)
# Compare accuracy or confusion matrix
table(Manual = manual_preds, BuiltIn = builtin_preds)
# Custom KNN function (k = 5 by default)
knn_manual <- function(train_x, train_y, test_x, k = 5) {
predict_y <- vector("character", nrow(test_x))
for (i in 1:nrow(test_x)) {
distances <- sqrt(rowSums((t(t(train_x) - test_x[i, ]))^2))
neighbors <- order(distances)[1:k]
labels <- train_y[neighbors]
predict_y[i] <- names(sort(table(labels), decreasing = TRUE))[1]
}
return(as.factor(predict_y))
}
# Run manual KNN
manual_preds <- knn_manual(train_x = dat[, 1:2],
train_y = dat$y,
test_x = test_dat[, 1:2],
k = 5)
# Load required package
library(class)
# Built-in knn()
builtin_preds <- knn(train = dat[, 1:2],
test = test_dat[, 1:2],
cl = dat$y,
k = 5)
# Compare accuracy or confusion matrix
table(Manual = manual_preds, BuiltIn = builtin_preds)
# Accuracy tracker
accuracies <- numeric(30)
# Loop through k = 1 to 30
for (k in 1:30) {
preds <- knn_manual(train_x = dat[, 1:2],
train_y = dat$y,
test_x = test_dat[, 1:2],
k = k)
acc <- mean(preds == test_dat$y)
accuracies[k] <- acc * 100  # convert to percentage
}
# Custom KNN function (k = 5 by default)
knn_manual <- function(train_x, train_y, test_x, k = 5) {
predict_y <- vector("character", nrow(test_x))
for (i in 1:nrow(test_x)) {
distances <- sqrt(rowSums((t(t(train_x) - test_x[i, ]))^2))
neighbors <- order(distances)[1:k]
labels <- train_y[neighbors]
predict_y[i] <- names(sort(table(labels), decreasing = TRUE))[1]
}
return(as.factor(predict_y))
}
# Run manual KNN
manual_preds <- knn_manual(train_x = dat[, 1:2],
train_y = dat$y,
test_x = test_dat[, 1:2],
k = 5)
# Load required package
library(class)
# Built-in knn()
builtin_preds <- knn(train = dat[, 1:2],
test = test_dat[, 1:2],
cl = dat$y,
k = 5)
# Compare accuracy or confusion matrix
table(Manual = manual_preds, BuiltIn = builtin_preds)
# Accuracy tracker
accuracies <- numeric(30)
# Loop through k = 1 to 30
for (k in 1:30) {
preds <- knn_manual(train_x = dat[, 1:2],
train_y = dat$y,
test_x = test_dat[, 1:2],
k = k)
acc <- mean(preds == test_dat$y)
accuracies[k] <- acc * 100  # convert to percentage
}
# Accuracy tracker
accuracies <- numeric(30)
# Ensure test labels and prediction levels match
true_labels <- factor(test_dat$y, levels = levels(dat$y))  # Align test labels to training levels
# Loop through k = 1 to 30
for (k in 1:30) {
preds <- knn_manual(train_x = dat[, 1:2],
train_y = dat$y,
test_x = test_dat[, 1:2],
k = k)
# Align prediction levels
preds <- factor(preds, levels = levels(dat$y))
# Compute accuracy
acc <- mean(preds == true_labels)
accuracies[k] <- acc * 100  # convert to percentage
}
# Plot accuracy vs. k
plot(1:30, accuracies, type = "b", pch = 19,
xlab = "Number of Neighbors (k)",
ylab = "Accuracy (%)",
main = "KNN Accuracy vs. k",
col = "blue")
