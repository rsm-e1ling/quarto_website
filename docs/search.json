[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\n\n\nEileen Ling\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nEileen Ling\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nEileen Ling\n\n\nApr 22, 2001\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/HW 3/index.html",
    "href": "blog/HW 3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/HW 3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/HW 3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/HW 3/index.html#simulate-conjoint-data",
    "href": "blog/HW 3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "blog/HW 3/index.html#preparing-the-data-for-estimation",
    "href": "blog/HW 3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nlibrary(dplyr)\n\n# Step 1: Load data\nconjoint_data &lt;- read.csv(\"conjoint_data.csv\")\n\n# Step 2: Set factor levels (reference categories: H for brand, No for ad)\nconjoint_data &lt;- conjoint_data %&gt;%\n  mutate(\n    brand = factor(brand, levels = c(\"H\", \"P\", \"N\")),\n    ad = factor(ad, levels = c(\"No\", \"Yes\"))\n  )\n\n# Step 3: Create dummy variables (drop reference category)\nconjoint_data &lt;- conjoint_data %&gt;%\n  mutate(\n    brand_P = as.integer(brand == \"P\"),\n    brand_N = as.integer(brand == \"N\"),\n    ad_Yes = as.integer(ad == \"Yes\")\n  )\n\n# Step 4: Create a unique identifier for each choice set\nconjoint_data &lt;- conjoint_data %&gt;%\n  mutate(choice_set = paste(resp, task, sep = \"_\"))\n\n# Step 5: Reorder columns\nconjoint_data &lt;- conjoint_data %&gt;%\n  select(resp, task, choice_set, choice, brand_P, brand_N, ad_Yes, price)\n\n# Step 6: Preview\nhead(conjoint_data)\n\n  resp task choice_set choice brand_P brand_N ad_Yes price\n1    1    1        1_1      1       0       1      1    28\n2    1    1        1_1      0       0       0      1    16\n3    1    1        1_1      0       1       0      1    16\n4    1    2        1_2      0       0       1      1    32\n5    1    2        1_2      1       1       0      1    16\n6    1    2        1_2      0       0       1      1    24\n\n\n::::"
  },
  {
    "objectID": "blog/HW 3/index.html#estimation-via-maximum-likelihood",
    "href": "blog/HW 3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe now estimate the part-worth utilities using Maximum Likelihood Estimation (MLE). Assuming that the data has already been reshaped and dummy-coded in Section 3, we extract the relevant variables, define the log-likelihood function for the MNL model, and use the BFGS algorithm to find the optimal coefficients.\n\nlibrary(dplyr)\n\n# Step 1: Prepare design matrix X and response y\nX &lt;- as.matrix(conjoint_data %&gt;% select(brand_P, brand_N, ad_Yes, price))\ny &lt;- conjoint_data$choice\nchoice_set &lt;- conjoint_data$choice_set\nset_ids &lt;- as.numeric(factor(choice_set))\nn_sets &lt;- length(unique(set_ids))\n\n# Step 2: Define negative log-likelihood function\nmnl_log_likelihood &lt;- function(beta) {\n  utilities &lt;- X %*% beta\n  exp_util &lt;- exp(utilities)\n  \n  denom &lt;- numeric(nrow(X))\n  for (s in 1:n_sets) {\n    mask &lt;- which(set_ids == s)\n    denom[mask] &lt;- sum(exp_util[mask])\n  }\n  \n  probs &lt;- exp_util / denom\n  log_lik &lt;- sum(y * log(probs + 1e-12))  # add small epsilon to avoid log(0)\n  return(-log_lik)\n}\n\n# Step 3: Run MLE using optim\ninit_beta &lt;- rep(0, ncol(X))\nresult &lt;- optim(par = init_beta, fn = mnl_log_likelihood, method = \"BFGS\", hessian = TRUE)\n\n# Step 4: Show estimated coefficients\ncoef_names &lt;- c(\"brand_P\", \"brand_N\", \"ad_Yes\", \"price\")\nsetNames(result$par, coef_names)\n\n    brand_P     brand_N      ad_Yes       price \n 0.50161701  0.94120473 -0.73200143 -0.09948157 \n\n\n::::\n\n# Compute standard errors from inverse Hessian\nse &lt;- sqrt(diag(solve(result$hessian)))\nz &lt;- 1.96\n\nconfint &lt;- data.frame(\n  Estimate = result$par,\n  Std.Error = se,\n  CI.Lower = result$par - z * se,\n  CI.Upper = result$par + z * se\n)\nrownames(confint) &lt;- coef_names\nconfint\n\n           Estimate   Std.Error   CI.Lower    CI.Upper\nbrand_P  0.50161701 0.111100000  0.2838610  0.71937301\nbrand_N  0.94120473 0.111039639  0.7235670  1.15884242\nad_Yes  -0.73200143 0.087809687 -0.9041084 -0.55989445\nprice   -0.09948157 0.006333649 -0.1118955 -0.08706762\n\n\n::::\nThis code computes the MLEs for the MNL model using scipy.optimize.minimize, extracts the covariance matrix from the inverse Hessian, calculates standard errors, and constructs 95% confidence intervals for each parameter:\n\nparam_names &lt;- c(\"brand_P\", \"brand_N\", \"ad_Yes\", \"price\")\n\nestimates &lt;- result$par\n\nhessian_inv &lt;- solve(result$hessian)\nse &lt;- sqrt(diag(hessian_inv))\n\nz &lt;- 1.96\nlower &lt;- estimates - z * se\nupper &lt;- estimates + z * se\n\nsummary &lt;- data.frame(\n  Estimate = estimates,\n  `Std. Error` = se,\n  `CI Lower (95%)` = lower,\n  `CI Upper (95%)` = upper,\n  row.names = param_names\n)\n\nsummary\n\n           Estimate  Std..Error CI.Lower..95.. CI.Upper..95..\nbrand_P  0.50161701 0.111100000      0.2838610     0.71937301\nbrand_N  0.94120473 0.111039639      0.7235670     1.15884242\nad_Yes  -0.73200143 0.087809687     -0.9041084    -0.55989445\nprice   -0.09948157 0.006333649     -0.1118955    -0.08706762\n\n\n::::"
  },
  {
    "objectID": "blog/HW 3/index.html#estimation-via-bayesian-methods",
    "href": "blog/HW 3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nThis code implements a Metropolis-Hastings MCMC sampler to estimate the posterior distribution of the four MNL parameters. It uses:\nA log-likelihood from the MNL model, Normal priors: N(0,5) for binary variables, N(0,1) for price, Independent normal proposals for each parameter.\nThe algorithm runs for 11,000 steps, discards the first 1,000 as burn-in, and summarizes the posterior means and 95% credible intervals for each parameter.\n\nX &lt;- as.matrix(conjoint_data %&gt;% select(brand_P, brand_N, ad_Yes, price))\ny &lt;- conjoint_data$choice\nchoice_set &lt;- conjoint_data$choice_set\nset_ids &lt;- as.numeric(factor(choice_set))\nn_sets &lt;- length(unique(set_ids))\n\n\nlog_likelihood &lt;- function(beta) {\n  utilities &lt;- X %*% beta\n  exp_util &lt;- exp(utilities)\n  \n  denom &lt;- numeric(nrow(X))\n  for (s in 1:n_sets) {\n    mask &lt;- which(set_ids == s)\n    denom[mask] &lt;- sum(exp_util[mask])\n  }\n  \n  probs &lt;- exp_util / denom\n  sum(y * log(probs + 1e-12))\n}\n\n\nlog_prior &lt;- function(beta) {\n  lp &lt;- -0.5 * sum(beta[1:3]^2 / 5 + log(5) * 3)\n  lp &lt;- lp - 0.5 * (beta[4]^2 / 1 + log(1))\n  lp\n}\n\n\nlog_posterior &lt;- function(beta) {\n  log_likelihood(beta) + log_prior(beta)\n}\n\n\nproposal_sd &lt;- c(0.05, 0.05, 0.05, 0.005)\n\n\nn_steps &lt;- 11000\nburn_in &lt;- 1000\nsamples &lt;- matrix(NA, nrow = n_steps, ncol = 4)\nsamples[1, ] &lt;- rep(0, 4)\naccepted &lt;- 0\n\n\nfor (t in 2:n_steps) {\n  current &lt;- samples[t - 1, ]\n  proposal &lt;- current + rnorm(4, mean = 0, sd = proposal_sd)\n  \n  log_post_current &lt;- log_posterior(current)\n  log_post_proposal &lt;- log_posterior(proposal)\n  \n  log_accept_ratio &lt;- log_post_proposal - log_post_current\n  \n  if (log(runif(1)) &lt; log_accept_ratio) {\n    samples[t, ] &lt;- proposal\n    accepted &lt;- accepted + 1\n  } else {\n    samples[t, ] &lt;- current\n  }\n}\n\n\nsamples_post &lt;- samples[(burn_in + 1):n_steps, ]\n\n\nparam_names &lt;- c(\"brand_P\", \"brand_N\", \"ad_Yes\", \"price\")\nposterior_summary &lt;- data.frame(\n  Mean = apply(samples_post, 2, mean),\n  `Lower 95%` = apply(samples_post, 2, quantile, 0.025),\n  `Upper 95%` = apply(samples_post, 2, quantile, 0.975)\n)\nrownames(posterior_summary) &lt;- param_names\n\nposterior_summary\n\n               Mean  Lower.95.  Upper.95.\nbrand_P  0.50067362  0.2560596  0.7246752\nbrand_N  0.94067500  0.7179513  1.1580759\nad_Yes  -0.72871233 -0.9056909 -0.5602154\nprice   -0.09968015 -0.1116646 -0.0880743\n\n\n::::\nThis code visualizes the MCMC results for the price coefficient by plotting its sampled values across iterations (trace plot) and showing the distribution of those values (posterior histogram) after burn-in:\n\nprice_samples &lt;- samples_post[, 4]\n\n\npar(mfrow = c(1, 2), mar = c(4, 4, 2, 1))\n\n# Trace plot\nplot(price_samples, type = \"l\", col = \"blue\", main = \"Trace Plot for β_price\",\n     xlab = \"Iteration\", ylab = \"Value\")\n\n# Histogram\nhist(price_samples, breaks = 40, col = \"skyblue\", border = \"black\",\n     main = \"Posterior Distribution of β_price\",\n     xlab = \"Value\", ylab = \"Frequency\")\n\n\n\n\nTrace plot and posterior histogram for β_price\n\n\n\n\n::::\nThis code summarizes the posterior statistics from the MCMC sampler and compares them side-by-side with the MLE results obtained earlier:\n\nparam_names &lt;- c(\"brand_P\", \"brand_N\", \"ad_Yes\", \"price\")\n\n# ---- Bayes ----\nposterior_mean &lt;- colMeans(samples_post)\nposterior_sd &lt;- apply(samples_post, 2, sd)\nposterior_ci_lower &lt;- apply(samples_post, 2, quantile, 0.025)\nposterior_ci_upper &lt;- apply(samples_post, 2, quantile, 0.975)\n\nposterior_stats &lt;- data.frame(\n  `Mean (Bayes)` = posterior_mean,\n  `Std Dev (Bayes)` = posterior_sd,\n  `Lower 95% (Bayes)` = posterior_ci_lower,\n  `Upper 95% (Bayes)` = posterior_ci_upper,\n  row.names = param_names\n)\n\n# ---- MLE ----\nmle_estimates &lt;- result$par\nhessian_inv &lt;- solve(result$hessian)\nmle_se &lt;- sqrt(diag(hessian_inv))\n\nmle_stats &lt;- data.frame(\n  `Mean (MLE)` = mle_estimates,\n  `Std Dev (MLE)` = mle_se,\n  `Lower 95% (MLE)` = mle_estimates - 1.96 * mle_se,\n  `Upper 95% (MLE)` = mle_estimates + 1.96 * mle_se,\n  row.names = param_names\n)\n\n\ncomparison &lt;- cbind(mle_stats, posterior_stats)\ncomparison\n\n         Mean..MLE. Std.Dev..MLE. Lower.95...MLE. Upper.95...MLE. Mean..Bayes.\nbrand_P  0.50161701   0.111100000       0.2838610      0.71937301   0.50067362\nbrand_N  0.94120473   0.111039639       0.7235670      1.15884242   0.94067500\nad_Yes  -0.73200143   0.087809687      -0.9041084     -0.55989445  -0.72871233\nprice   -0.09948157   0.006333649      -0.1118955     -0.08706762  -0.09968015\n        Std.Dev..Bayes. Lower.95...Bayes. Upper.95...Bayes.\nbrand_P     0.116980472         0.2560596         0.7246752\nbrand_N     0.112625398         0.7179513         1.1580759\nad_Yes      0.088558003        -0.9056909        -0.5602154\nprice       0.006121052        -0.1116646        -0.0880743\n\n\n::::"
  },
  {
    "objectID": "blog/HW 3/index.html#discussion",
    "href": "blog/HW 3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we had not simulated the data and were instead analyzing real-world survey responses, we would interpret the parameter estimates as reflecting consumer preferences inferred from observed choices.\nFor example, if the estimate for β_Netflix is greater than β_Prime, this indicates that — holding other attributes constant — consumers exhibit a stronger preference for Netflix over Amazon Prime. In a utility-maximizing framework, higher part-worths translate to higher probabilities of choice.\nSimilarly, the negative estimate for β_price is economically intuitive. It implies that, all else equal, increasing the monthly price reduces the utility of a streaming service — and thus makes it less likely to be chosen. A negative price coefficient is a typical and expected result in discrete choice modeling.\nOverall, these signs and magnitudes align with what we would expect in practice: consumers prefer premium brands and ad-free experiences, but are sensitive to price. The statistical and Bayesian consistency across the estimation methods provides further confidence in these findings.\nIn real-world applications, individuals often differ in their preferences — some may be highly price-sensitive, while others care more about brand or ad-free experiences. A multi-level or hierarchical (aka mixed or random-parameter) logit model captures this heterogeneity by allowing each respondent to have their own set of preference parameters.\nTo simulate data from such a model, and to estimate it, we would make the following key changes:\n\nData Simulation Changes\nInstead of drawing a single set of part-worths β, we simulate a distribution over betas: 𝛽_𝑖 ∼ N(μ,Σ) where 𝜇 is the population-level mean vector, and Σ is the covariance matrix of individual differences. For each respondent 𝑖, we sample β_i and use it to generate their choices across all tasks.\n\n\nEstimation Changes\nThe likelihood becomes a marginal likelihood, integrating over each individual’s latent β_i. This often requires: Bayesian methods like Gibbs sampling or Hamiltonian Monte Carlo, or Simulation-based frequentist methods like simulated maximum likelihood using Halton or Sobol draws. We now estimate: The population-level means μ The variances and possibly covariances in Σ\n\n\nIntuition\nThis model captures individual-level heterogeneity, providing more personalized insights. It improves predictive accuracy in applied settings like product design and pricing because it doesn’t assume all consumers behave identically."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eileen_Website",
    "section": "",
    "text": "Heyyyy!! This is Eileen Ling and you can also call me Jiawei 凌家蔚! I’m from Shanghai, China and now study in University of California, San Diego, majoring in Business Analytics."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/HW 2/index.html",
    "href": "blog/HW 2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\n\nTo compare the patent success of firms using Blueprinty’s software versus those that do not, I first look at the histograms for both groups. The histograms show the distribution of patents awarded, with separate colors indicating whether the firm is a customer or not. I also calculate the mean number of patents for both groups. This allows us to compare the average success between software users and non-users and assess whether Blueprinty’s software correlates with higher patent approval rates.\n\n\n\n\n\n\n\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n# A tibble: 2 × 2\n  iscustomer mean_patents\n       &lt;int&gt;        &lt;dbl&gt;\n1          0         3.47\n2          1         4.13\n\n\nThe histogram shows that firms using Blueprinty’s software tend to have more patents than non-users. The average number of patents for software users is 4.13, compared to 3.47 for non-users, suggesting that the software may be associated with higher patent success.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo compare the regions and ages between Blueprinty customers and non-customers, I begin by summarizing the data to see how these variables differ across the two groups. By examining the regional distribution and age, we can gain insights into whether there are systematic differences between customers and non-customers.\n\nCompare the regional distribution:\n\n\n\n\n\n\n\n\n\n\nIn the Northeast, non-customers (red) significantly outnumber customers (blue), while in the Midwest, customers and non-customers are more balanced. The South has a relatively higher number of non-customers, but the Southwest shows a clear dominance of customers over non-customers.\nThis suggests that the adoption of Blueprinty’s software is not evenly distributed across regions, with certain regions (like the Southwest) showing a higher proportion of customers, while others (like the Northeast) have more non-customers.\n\nCompare ages:\n\n\n\n# A tibble: 2 × 4\n  iscustomer mean_age median_age age_range\n       &lt;int&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n1          0     26.1       25.5 9 - 47.5 \n2          1     26.9       26.5 10 - 49  \n\n\nRegarding age, the average and median ages for both customers and non-customers are similar, with non-customers having an average age of 26.1 and customers 26.9. The age range is also nearly the same, indicating that age is not a major factor influencing the adoption of Blueprinty’s software.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, I use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. I start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for ( Y () ) is given by:\n\\[\nf(Y | \\lambda) = \\frac{\\lambda^Y e^{-\\lambda}}{Y!}\n\\]\nWhere: - ( Y ) is the number of patents awarded to a firm, - lambda is the rate parameter (mean number of patents), and - ( Y! ) is the factorial of ( Y ).\nFor a sample of independent observations ( Y_1, Y_2, , Y_n ), the likelihood function is the product of the individual likelihoods:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{Y_i} e^{-\\lambda}}{Y_i!}\n\\]\nWhere ( n ) is the number of firms in the sample.\nTo estimate lambda , we maximize the log-likelihood:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( Y_i \\log \\lambda - \\lambda - \\log(Y_i!) \\right)\n\\]\nMaximizing the log-likelihood will provide the estimate for lambda, which represents the average number of patents awarded across all firms in the dataset.\nI then use this function to calculate the log-likelihood for a given value of 𝜆 in Poisson model and a vector of observed patent counts.\n\n# Define the log-likelihood function for the Poisson model\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  # Calculate the log-likelihood\n  log_likelihood &lt;- sum(Y * log(lambda) - lambda - log(factorial(Y)))\n  \n  # Return the log-likelihood\n  return(log_likelihood)\n}\n\n\n# Define the observed number of patents \n# For demonstration, using a sample set of data\nY &lt;- data$patents  \n\n# Define a range of lambda values\nlambda_range &lt;- seq(0.1, 10, by = 0.1)  # Range of lambda values from 0.1 to 10\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihood_values &lt;- sapply(lambda_range, function(lambda) poisson_loglikelihood(lambda, Y))\n\n# Plot the log-likelihood function\nplot(lambda_range, log_likelihood_values, type = \"l\", \n     main = \"Log-Likelihood vs Lambda\", \n     xlab = \"Lambda\", \n     ylab = \"Log-Likelihood\", \n     col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\nThe plot shows the log-likelihood function for different values of𝜆, representing the average number of patents. It increases sharply at first, then levels off and decreases as 𝜆 continues to rise. The peak of the curve indicates the optimal value of 𝜆, which corresponds to the maximum likelihood estimate for the average number of patents awarded.\nTo find the Maximum Likelihood Estimate (MLE) for 𝜆 in a Poisson distribution, we take the first derivative of the log-likelihood function with respect to 𝜆, set it equal to zero, and solve for 𝜆.\nThe log-likelihood function is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( Y_i \\log \\lambda - \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the first derivative with respect to 𝜆:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( \\frac{Y_i}{\\lambda} - 1 \\right)\n\\]\nSetting this equal to zero:\n\\[\n\\sum_{i=1}^{n} \\left( \\frac{Y_i}{\\lambda} - 1 \\right) = 0\n\\]\nSimplifying:\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThus, the MLE for 𝜆 is the sample mean of the observed values Y , denoted bar_Y. This is consistent with the fact that the mean of a Poisson distribution is 𝜆.\n\n# Define the log-likelihood function for the Poisson model\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  log_likelihood &lt;- sum(Y * log(lambda) - lambda - log(factorial(Y)))\n  return(log_likelihood)  # Return log-likelihood\n}\n\n# Define the observed number of patents (Y)\nY &lt;- data$patents  # Replace 'data$patents' with your actual data column\n\n# Use optim() to find the MLE for lambda using the Brent method\nmle_result &lt;- optim(par = 1, fn = function(l) -poisson_loglikelihood(l, Y),\n                    method = \"Brent\", lower = 0.01, upper = 20)\n\n# Extract the MLE for lambda\nlambda_mle &lt;- mle_result$par\nlambda_mle\n\n[1] 3.684667\n\n\nThe MLE of λ is 3.684667, meaning that the estimated rate of patents awarded per firm over the last 5 years is approximately 3.68.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nTo estimate the MLE vector and the Hessian of the Poisson regression model with covariates using R’s optim() function, and then calculate the standard errors, follow the steps below. We’ll use the Hessian matrix from the optimization result to compute the standard errors of the coefficient estimates.\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n             Term     Estimate     StdError\n1       intercept -0.057869390 0.0247676826\n2             age  0.111376215 0.0034735079\n3          age_sq -0.002175247 0.0000530556\n4 regionNortheast  0.001218348 0.0375689502\n5 regionNorthwest -0.007435508 0.0489787049\n6     regionSouth -0.017080725 0.0486594534\n7 regionSouthwest  0.004882447 0.0430424130\n8      iscustomer  0.000000000 0.0000000000\n\n\nThen I check my results using R’s glm() function.\n\n\n\nCall:\nglm(formula = Y ~ X, family = poisson(link = \"log\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.9781  -0.9278  -0.1149   0.5924   5.0424  \n\nCoefficients: (2 not defined because of singularities)\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.3843338  0.1820919  -2.111  0.03480 *  \nXintercept               NA         NA      NA       NA    \nXage              0.1407257  0.0138101  10.190  &lt; 2e-16 ***\nXage_sq          -0.0028103  0.0002568 -10.944  &lt; 2e-16 ***\nXregionNortheast  0.1121092  0.0417496   2.685  0.00725 ** \nXregionNorthwest -0.0193872  0.0537827  -0.360  0.71849    \nXregionSouth      0.0609820  0.0526598   1.158  0.24685    \nXregionSouthwest  0.0544801  0.0472012   1.154  0.24841    \nXiscustomer              NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2187.8  on 1493  degrees of freedom\nAIC: 6574.7\n\nNumber of Fisher Scoring iterations: 5\n\n\nInterpret the results:\nThe Poisson regression model shows the following:\nSignificant predictors:\nAge (Xage) has a positive effect on the outcome, with each additional year increasing the log of the expected count.\nAge squared (Xage_sq) has a negative effect, indicating a diminishing relationship as age increases.\nRegion (Northeast) (XregionNortheast) is significant, suggesting that being in the Northeast increases the expected count.\nInsignificant predictors:\nOther regions (Northwest, South, Southwest) and customer status (Xiscustomer) are not statistically significant.\nModel fit: The model improves on the null model, as indicated by the decrease in deviance, though there’s still room for improvement.\nTo assess the impact of Blueprinty’s software on patent success, we simulate two scenarios: one where firms do not use the software (X_0, with iscustomer = 0), and one where they do (X_1, with iscustomer = 1). We use the fitted Poisson regression model to predict the number of patents for firms in both scenarios, obtaining y_pred_0 and y_pred_1, respectively.\nWe then compute the average of this difference. A positive average suggests that using the software increases patent success, while a negative average suggests a decrease. An average close to zero indicates no significant effect.\nThis method estimates the average impact of Blueprinty’s software on patent success by comparing the predicted outcomes for firms with and without the software."
  },
  {
    "objectID": "blog/HW 2/index.html#blueprinty-case-study",
    "href": "blog/HW 2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\n\nTo compare the patent success of firms using Blueprinty’s software versus those that do not, I first look at the histograms for both groups. The histograms show the distribution of patents awarded, with separate colors indicating whether the firm is a customer or not. I also calculate the mean number of patents for both groups. This allows us to compare the average success between software users and non-users and assess whether Blueprinty’s software correlates with higher patent approval rates.\n\n\n\n\n\n\n\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n# A tibble: 2 × 2\n  iscustomer mean_patents\n       &lt;int&gt;        &lt;dbl&gt;\n1          0         3.47\n2          1         4.13\n\n\nThe histogram shows that firms using Blueprinty’s software tend to have more patents than non-users. The average number of patents for software users is 4.13, compared to 3.47 for non-users, suggesting that the software may be associated with higher patent success.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo compare the regions and ages between Blueprinty customers and non-customers, I begin by summarizing the data to see how these variables differ across the two groups. By examining the regional distribution and age, we can gain insights into whether there are systematic differences between customers and non-customers.\n\nCompare the regional distribution:\n\n\n\n\n\n\n\n\n\n\nIn the Northeast, non-customers (red) significantly outnumber customers (blue), while in the Midwest, customers and non-customers are more balanced. The South has a relatively higher number of non-customers, but the Southwest shows a clear dominance of customers over non-customers.\nThis suggests that the adoption of Blueprinty’s software is not evenly distributed across regions, with certain regions (like the Southwest) showing a higher proportion of customers, while others (like the Northeast) have more non-customers.\n\nCompare ages:\n\n\n\n# A tibble: 2 × 4\n  iscustomer mean_age median_age age_range\n       &lt;int&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n1          0     26.1       25.5 9 - 47.5 \n2          1     26.9       26.5 10 - 49  \n\n\nRegarding age, the average and median ages for both customers and non-customers are similar, with non-customers having an average age of 26.1 and customers 26.9. The age range is also nearly the same, indicating that age is not a major factor influencing the adoption of Blueprinty’s software.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, I use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. I start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for ( Y () ) is given by:\n\\[\nf(Y | \\lambda) = \\frac{\\lambda^Y e^{-\\lambda}}{Y!}\n\\]\nWhere: - ( Y ) is the number of patents awarded to a firm, - lambda is the rate parameter (mean number of patents), and - ( Y! ) is the factorial of ( Y ).\nFor a sample of independent observations ( Y_1, Y_2, , Y_n ), the likelihood function is the product of the individual likelihoods:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{Y_i} e^{-\\lambda}}{Y_i!}\n\\]\nWhere ( n ) is the number of firms in the sample.\nTo estimate lambda , we maximize the log-likelihood:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( Y_i \\log \\lambda - \\lambda - \\log(Y_i!) \\right)\n\\]\nMaximizing the log-likelihood will provide the estimate for lambda, which represents the average number of patents awarded across all firms in the dataset.\nI then use this function to calculate the log-likelihood for a given value of 𝜆 in Poisson model and a vector of observed patent counts.\n\n# Define the log-likelihood function for the Poisson model\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  # Calculate the log-likelihood\n  log_likelihood &lt;- sum(Y * log(lambda) - lambda - log(factorial(Y)))\n  \n  # Return the log-likelihood\n  return(log_likelihood)\n}\n\n\n# Define the observed number of patents \n# For demonstration, using a sample set of data\nY &lt;- data$patents  \n\n# Define a range of lambda values\nlambda_range &lt;- seq(0.1, 10, by = 0.1)  # Range of lambda values from 0.1 to 10\n\n# Calculate the log-likelihood for each lambda value\nlog_likelihood_values &lt;- sapply(lambda_range, function(lambda) poisson_loglikelihood(lambda, Y))\n\n# Plot the log-likelihood function\nplot(lambda_range, log_likelihood_values, type = \"l\", \n     main = \"Log-Likelihood vs Lambda\", \n     xlab = \"Lambda\", \n     ylab = \"Log-Likelihood\", \n     col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\nThe plot shows the log-likelihood function for different values of𝜆, representing the average number of patents. It increases sharply at first, then levels off and decreases as 𝜆 continues to rise. The peak of the curve indicates the optimal value of 𝜆, which corresponds to the maximum likelihood estimate for the average number of patents awarded.\nTo find the Maximum Likelihood Estimate (MLE) for 𝜆 in a Poisson distribution, we take the first derivative of the log-likelihood function with respect to 𝜆, set it equal to zero, and solve for 𝜆.\nThe log-likelihood function is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( Y_i \\log \\lambda - \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the first derivative with respect to 𝜆:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_{i=1}^{n} \\left( \\frac{Y_i}{\\lambda} - 1 \\right)\n\\]\nSetting this equal to zero:\n\\[\n\\sum_{i=1}^{n} \\left( \\frac{Y_i}{\\lambda} - 1 \\right) = 0\n\\]\nSimplifying:\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThus, the MLE for 𝜆 is the sample mean of the observed values Y , denoted bar_Y. This is consistent with the fact that the mean of a Poisson distribution is 𝜆.\n\n# Define the log-likelihood function for the Poisson model\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  log_likelihood &lt;- sum(Y * log(lambda) - lambda - log(factorial(Y)))\n  return(log_likelihood)  # Return log-likelihood\n}\n\n# Define the observed number of patents (Y)\nY &lt;- data$patents  # Replace 'data$patents' with your actual data column\n\n# Use optim() to find the MLE for lambda using the Brent method\nmle_result &lt;- optim(par = 1, fn = function(l) -poisson_loglikelihood(l, Y),\n                    method = \"Brent\", lower = 0.01, upper = 20)\n\n# Extract the MLE for lambda\nlambda_mle &lt;- mle_result$par\nlambda_mle\n\n[1] 3.684667\n\n\nThe MLE of λ is 3.684667, meaning that the estimated rate of patents awarded per firm over the last 5 years is approximately 3.68.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nTo estimate the MLE vector and the Hessian of the Poisson regression model with covariates using R’s optim() function, and then calculate the standard errors, follow the steps below. We’ll use the Hessian matrix from the optimization result to compute the standard errors of the coefficient estimates.\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n             Term     Estimate     StdError\n1       intercept -0.057869390 0.0247676826\n2             age  0.111376215 0.0034735079\n3          age_sq -0.002175247 0.0000530556\n4 regionNortheast  0.001218348 0.0375689502\n5 regionNorthwest -0.007435508 0.0489787049\n6     regionSouth -0.017080725 0.0486594534\n7 regionSouthwest  0.004882447 0.0430424130\n8      iscustomer  0.000000000 0.0000000000\n\n\nThen I check my results using R’s glm() function.\n\n\n\nCall:\nglm(formula = Y ~ X, family = poisson(link = \"log\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.9781  -0.9278  -0.1149   0.5924   5.0424  \n\nCoefficients: (2 not defined because of singularities)\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.3843338  0.1820919  -2.111  0.03480 *  \nXintercept               NA         NA      NA       NA    \nXage              0.1407257  0.0138101  10.190  &lt; 2e-16 ***\nXage_sq          -0.0028103  0.0002568 -10.944  &lt; 2e-16 ***\nXregionNortheast  0.1121092  0.0417496   2.685  0.00725 ** \nXregionNorthwest -0.0193872  0.0537827  -0.360  0.71849    \nXregionSouth      0.0609820  0.0526598   1.158  0.24685    \nXregionSouthwest  0.0544801  0.0472012   1.154  0.24841    \nXiscustomer              NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2187.8  on 1493  degrees of freedom\nAIC: 6574.7\n\nNumber of Fisher Scoring iterations: 5\n\n\nInterpret the results:\nThe Poisson regression model shows the following:\nSignificant predictors:\nAge (Xage) has a positive effect on the outcome, with each additional year increasing the log of the expected count.\nAge squared (Xage_sq) has a negative effect, indicating a diminishing relationship as age increases.\nRegion (Northeast) (XregionNortheast) is significant, suggesting that being in the Northeast increases the expected count.\nInsignificant predictors:\nOther regions (Northwest, South, Southwest) and customer status (Xiscustomer) are not statistically significant.\nModel fit: The model improves on the null model, as indicated by the decrease in deviance, though there’s still room for improvement.\nTo assess the impact of Blueprinty’s software on patent success, we simulate two scenarios: one where firms do not use the software (X_0, with iscustomer = 0), and one where they do (X_1, with iscustomer = 1). We use the fitted Poisson regression model to predict the number of patents for firms in both scenarios, obtaining y_pred_0 and y_pred_1, respectively.\nWe then compute the average of this difference. A positive average suggests that using the software increases patent success, while a negative average suggests a decrease. An average close to zero indicates no significant effect.\nThis method estimates the average impact of Blueprinty’s software on patent success by comparing the predicted outcomes for firms with and without the software."
  },
  {
    "objectID": "blog/HW 2/index.html#airbnb-case-study",
    "href": "blog/HW 2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nAssume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\nRows: 40,628\nColumns: 14\n$ X                         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ id                        &lt;int&gt; 2515, 2595, 3647, 3831, 4611, 5099, 5107, 51…\n$ days                      &lt;int&gt; 3130, 3127, 3050, 3038, 3012, 2981, 2981, 29…\n$ last_scraped              &lt;chr&gt; \"4/2/2017\", \"4/2/2017\", \"4/2/2017\", \"4/2/201…\n$ host_since                &lt;chr&gt; \"9/6/2008\", \"9/9/2008\", \"11/25/2008\", \"12/7/…\n$ room_type                 &lt;chr&gt; \"Private room\", \"Entire home/apt\", \"Private …\n$ bathrooms                 &lt;dbl&gt; 1, 1, 1, 1, NA, 1, 1, NA, 1, 1, 1, 1, 1, NA,…\n$ bedrooms                  &lt;int&gt; 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ price                     &lt;int&gt; 59, 230, 150, 89, 39, 212, 250, 60, 129, 79,…\n$ number_of_reviews         &lt;int&gt; 150, 20, 0, 116, 93, 60, 60, 50, 53, 329, 11…\n$ review_scores_cleanliness &lt;int&gt; 9, 9, NA, 9, 9, 9, 10, 8, 9, 7, 10, 9, 9, 9,…\n$ review_scores_location    &lt;int&gt; 9, 10, NA, 9, 8, 9, 9, 9, 10, 10, 10, 9, 10,…\n$ review_scores_value       &lt;int&gt; 9, 9, NA, 9, 9, 9, 10, 9, 9, 9, 10, 9, 10, 9…\n$ instant_bookable          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"t\", \"f\", \"f\", \"f\", \"f\",…\n\n\n       X               id                days       last_scraped      \n Min.   :    1   Min.   :    2515   Min.   :    1   Length:40628      \n 1st Qu.:10158   1st Qu.: 4889868   1st Qu.:  542   Class :character  \n Median :20314   Median : 9862878   Median :  996   Mode  :character  \n Mean   :20314   Mean   : 9698889   Mean   : 1102                     \n 3rd Qu.:30471   3rd Qu.:14667894   3rd Qu.: 1535                     \n Max.   :40628   Max.   :18009669   Max.   :42828                     \n                                                                      \n  host_since         room_type           bathrooms        bedrooms     \n Length:40628       Length:40628       Min.   :0.000   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.: 1.000  \n Mode  :character   Mode  :character   Median :1.000   Median : 1.000  \n                                       Mean   :1.125   Mean   : 1.147  \n                                       3rd Qu.:1.000   3rd Qu.: 1.000  \n                                       Max.   :8.000   Max.   :10.000  \n                                       NA's   :160     NA's   :76      \n     price         number_of_reviews review_scores_cleanliness\n Min.   :   10.0   Min.   :  0.0     Min.   : 2.000           \n 1st Qu.:   70.0   1st Qu.:  1.0     1st Qu.: 9.000           \n Median :  100.0   Median :  4.0     Median :10.000           \n Mean   :  144.8   Mean   : 15.9     Mean   : 9.198           \n 3rd Qu.:  170.0   3rd Qu.: 17.0     3rd Qu.:10.000           \n Max.   :10000.0   Max.   :421.0     Max.   :10.000           \n                                     NA's   :10195            \n review_scores_location review_scores_value instant_bookable  \n Min.   : 2.000         Min.   : 2.000      Length:40628      \n 1st Qu.: 9.000         1st Qu.: 9.000      Class :character  \n Median :10.000         Median :10.000      Mode  :character  \n Mean   : 9.414         Mean   : 9.332                        \n 3rd Qu.:10.000         3rd Qu.:10.000                        \n Max.   :10.000         Max.   :10.000                        \n NA's   :10254          NA's   :10256                         \n\n\nData cleaning:\n\n\n       X               id                days       last_scraped      \n Min.   :    1   Min.   :    2515   Min.   :    7   Length:30160      \n 1st Qu.: 8630   1st Qu.: 4276690   1st Qu.:  584   Class :character  \n Median :18236   Median : 9149028   Median : 1041   Mode  :character  \n Mean   :18679   Mean   : 8978287   Mean   : 1140                     \n 3rd Qu.:28532   3rd Qu.:13914758   3rd Qu.: 1592                     \n Max.   :40504   Max.   :17973686   Max.   :42828                     \n  host_since                  room_type       bathrooms        bedrooms     \n Length:30160       Entire home/apt:15543   Min.   :0.000   Min.   : 0.000  \n Class :character   Private room   :13773   1st Qu.:1.000   1st Qu.: 1.000  \n Mode  :character   Shared room    :  844   Median :1.000   Median : 1.000  \n                                            Mean   :1.122   Mean   : 1.151  \n                                            3rd Qu.:1.000   3rd Qu.: 1.000  \n                                            Max.   :6.000   Max.   :10.000  \n     price         number_of_reviews review_scores_cleanliness\n Min.   :   10.0   Min.   :  1.00    Min.   : 2.000           \n 1st Qu.:   70.0   1st Qu.:  3.00    1st Qu.: 9.000           \n Median :  103.0   Median :  8.00    Median :10.000           \n Mean   :  140.2   Mean   : 21.17    Mean   : 9.202           \n 3rd Qu.:  169.0   3rd Qu.: 26.00    3rd Qu.:10.000           \n Max.   :10000.0   Max.   :421.00    Max.   :10.000           \n review_scores_location review_scores_value instant_bookable\n Min.   : 2.000         Min.   : 2.000      Min.   :0.0000  \n 1st Qu.: 9.000         1st Qu.: 9.000      1st Qu.:0.0000  \n Median :10.000         Median :10.000      Median :0.0000  \n Mean   : 9.415         Mean   : 9.334      Mean   :0.1962  \n 3rd Qu.:10.000         3rd Qu.:10.000      3rd Qu.:0.0000  \n Max.   :10.000         Max.   :10.000      Max.   :1.0000  \n\n\nDistribution of Reviews:\n\n\n\n\n\n\n\n\n\nPoisson Regression Model:\n\n\n\nCall:\nglm(formula = number_of_reviews ~ days + price + bedrooms + bathrooms + \n    review_scores_cleanliness + review_scores_location + review_scores_value + \n    instant_bookable + room_type, family = poisson(link = \"log\"), \n    data = airbnb_processed)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-21.096   -4.804   -3.001    0.961   38.630  \n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.498e+00  1.609e-02 217.396  &lt; 2e-16 ***\ndays                       5.072e-05  3.909e-07 129.757  &lt; 2e-16 ***\nprice                     -1.791e-05  8.327e-06  -2.151 0.031485 *  \nbedrooms                   7.409e-02  1.992e-03  37.197  &lt; 2e-16 ***\nbathrooms                 -1.177e-01  3.749e-03 -31.394  &lt; 2e-16 ***\nreview_scores_cleanliness  1.131e-01  1.496e-03  75.611  &lt; 2e-16 ***\nreview_scores_location    -7.690e-02  1.609e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.108e-02  1.804e-03 -50.490  &lt; 2e-16 ***\ninstant_bookable           3.459e-01  2.890e-03 119.666  &lt; 2e-16 ***\nroom_typePrivate room     -1.054e-02  2.738e-03  -3.847 0.000119 ***\nroom_typeShared room      -2.463e-01  8.620e-03 -28.578  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 926886  on 30149  degrees of freedom\nAIC: 1048375\n\nNumber of Fisher Scoring iterations: 9\n\n\nConclusion The Poisson regression model provides insights into the factors influencing the number of reviews for Airbnb listings in New York City. Several variables were found to significantly affect the number of reviews:\nDays Listed: A positive relationship between the number of days a property has been listed and the number of reviews suggests that longer-listed properties tend to accumulate more reviews.\nPrice: A negative coefficient for price indicates that higher-priced listings tend to receive fewer reviews, though the effect is relatively small.\nBedrooms and Bathrooms: More bedrooms are associated with more reviews, whereas more bathrooms are linked to fewer reviews, possibly reflecting the impact of property size on review frequency.\nReview Scores: Listings with higher cleanliness and location scores are likely to receive more reviews, while lower value scores negatively affect the number of reviews.\nInstant Bookable: The instant bookable feature has a strong positive effect on reviews, suggesting that properties with this feature attract more bookings and reviews.\nRoom Type: Shared rooms show a significant decrease in reviews compared to entire homes or private rooms, implying that room type plays an important role in attracting reviews.\nThe model’s fit, with a residual deviance of 926,886 and an AIC of 1,048,375, indicates that it explains a substantial portion of the variation in the number of reviews. Most predictors, except for price and room type, are statistically significant, underscoring their importance in determining the number of reviews.\nIn conclusion, Airbnb hosts can benefit from improving listing duration, cleanliness, location, and offering features like instant booking to increase their number of reviews. Additionally, adjusting price and room type may also have a meaningful impact on review frequency."
  },
  {
    "objectID": "blog/HW 1/index.html",
    "href": "blog/HW 1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study conducted by Dean Karlan and John List explored the effectiveness of matching grants in increasing charitable donations through a natural field experiment with over 50,000 prior donors to a nonprofit organization. Donors were randomly assigned to receive one of three match ratios (1:1, 2:1, or 3:1) or no match (control group).\nKey findings:\nMatching Grants Increased Donations: The matching grant increased both the response rate (22%) and revenue per solicitation (19%) compared to the control group.\nNo Impact of Larger Match Ratios: Larger match ratios (2:1 or 3:1) had no additional effect compared to the 1:1 match.\nPolitical Environment Influence: Donors in red states (Bush voters) responded more positively to the match, with a 55% increase in revenue,blue states (Kerry voters) was minimal.\nElasticity of Giving: Price elasticity (sensitivity to the matching offer) was estimated at -0.30, indicating moderate responsiveness to the match.\nConclusion: The presence of a matching grant boosts donations, but larger match ratios do not lead to significantly more contributions. Political factors also play a role in donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/HW 1/index.html#introduction",
    "href": "blog/HW 1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study conducted by Dean Karlan and John List explored the effectiveness of matching grants in increasing charitable donations through a natural field experiment with over 50,000 prior donors to a nonprofit organization. Donors were randomly assigned to receive one of three match ratios (1:1, 2:1, or 3:1) or no match (control group).\nKey findings:\nMatching Grants Increased Donations: The matching grant increased both the response rate (22%) and revenue per solicitation (19%) compared to the control group.\nNo Impact of Larger Match Ratios: Larger match ratios (2:1 or 3:1) had no additional effect compared to the 1:1 match.\nPolitical Environment Influence: Donors in red states (Bush voters) responded more positively to the match, with a 55% increase in revenue,blue states (Kerry voters) was minimal.\nElasticity of Giving: Price elasticity (sensitivity to the matching offer) was estimated at -0.30, indicating moderate responsiveness to the match.\nConclusion: The presence of a matching grant boosts donations, but larger match ratios do not lead to significantly more contributions. Political factors also play a role in donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/HW 1/index.html#data",
    "href": "blog/HW 1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset contains information on participants in a field experiment investigating charitable donations. Key columns include treatment and control, which identify whether individuals received a matching grant or were in the control group (no match). The ratio, ratio2, and ratio3 columns represent the match ratios in different treatment groups, such as 1:1, 2:1, and 3:1, with “Control” indicating no match. Additional columns like size, size25, size50, and size100 indicate the maximum matching grant amounts available, such as $100,000 or unspecified amounts. Redcty and bluecty reflect whether the donor resides in a Republican (red) or Democratic (blue) county, capturing the political environment. The dataset also includes demographic information about the donor’s area, such as pwhite and pblack, which represent the proportions of white and black residents, and page18_39, indicating the proportion of individuals aged 18-39. Other variables include ave_hh_sz (average household size), median_hhincome (median household income), powner (proportion of homeowners), psch_atlstba (proportion with at least a bachelor’s degree), and pop_propurban, which shows whether the donor lives in an urban area. This data provides insights into both the treatment characteristics and the demographic context of the participants, essential for understanding how different factors influenced charitable giving.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nfrom scipy import stats\n\n# Conducting the balance test for the variable 'mrm2' (months since last donation)\n\n# 1. T-test for treatment vs. control group on 'mrm2'\ntreatment_group = data[data['treatment'] == 1]['mrm2'].dropna()\ncontrol_group = data[data['treatment'] == 0]['mrm2'].dropna()\n\nt_stat, p_value_ttest = stats.ttest_ind(treatment_group, control_group)\n\n# 2. Linear regression: Regress 'mrm2' on treatment\nimport statsmodels.api as sm\n\n# Add constant term to the model\nX = sm.add_constant(data['treatment'])\ny = data['mrm2']\n\n# Linear regression model\nmodel = sm.OLS(y, X, missing='drop').fit()\nregression_coef = model.params['treatment']\np_value_regression = model.pvalues['treatment']\n\n# Results\nt_stat, p_value_ttest, regression_coef, p_value_regression\nHere are the results from both the t-test and the linear regression for the variable ‘mrm2’ (months since the last donation):\nT-test: T-statistic: 0.12 P-value: 0.90\nLinear Regression: Estimated Coefficient for Treatment: 0.0137 P-value for Treatment: 0.90\nInterpretation: Both the t-test and the linear regression indicate that there is no significant difference between the treatment and control groups with respect to the number of months since their last donation. This is because the p-value is much greater than the 0.05 significance level, meaning we fail to reject the null hypothesis of no difference.\nWhy Table 1 is Included in the Paper: Table 1 in the paper serves as a balance test to confirm that the randomization process was effective, i.e., the treatment and control groups are similar in terms of observable characteristics before the experiment. Ensuring there are no significant pre-existing differences between the groups is critical for the validity of the experiment, as it suggests that any post-treatment differences in outcomes can be attributed to the treatment itself, not to systematic differences between the groups."
  },
  {
    "objectID": "blog/HW 1/index.html#experimental-results",
    "href": "blog/HW 1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nimport matplotlib.pyplot as plt\n# Calculate the proportion of people who donated for treatment and control groups\nproportion_donated_treatment = data[data['treatment'] == 1]['gave'].mean()\nproportion_donated_control = data[data['treatment'] == 0]['gave'].mean()\n\n# Create a bar plot\nlabels = ['Treatment', 'Control']\nproportions = [proportion_donated_treatment, proportion_donated_control]\n\nplt.bar(labels, proportions, color=['blue', 'orange'])\nplt.xlabel('Group')\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of People Who Donated (Treatment vs. Control)')\nplt.ylim(0, 0.025)  # Adjust y-axis limit for better visibility\nplt.show()\n\n\n\nProportion of People Who Donated\n\n\nThen I run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding.\n# Conducting a t-test on the binary outcome 'gave' (whether any donation was made) for treatment and control groups\n\n# Treatment vs Control for the binary outcome of whether a donation was made\ntreatment_donated = data[data['treatment'] == 1]['gave'].dropna()\ncontrol_donated = data[data['treatment'] == 0]['gave'].dropna()\n\n# T-test\nt_stat_donated, p_value_donated = stats.ttest_ind(treatment_donated, control_donated)\n\n# Running a bivariate linear regression for the same binary outcome (gave) as dependent variable and treatment as independent variable\nX_donated = sm.add_constant(data['treatment'])  # adding constant for intercept\ny_donated = data['gave']\n\n# Linear regression model\nmodel_donated = sm.OLS(y_donated, X_donated, missing='drop').fit()\nregression_coef_donated = model_donated.params['treatment']\np_value_regression_donated = model_donated.pvalues['treatment']\n\n# Results\nt_stat_donated, p_value_donated, regression_coef_donated, p_value_regression_donated\nStatistical Results: T-test: The t-statistic is approximately 3.10, with a p-value of 0.0019.\nLinear Regression: The estimated coefficient for the treatment variable is approximately 0.0042, with a p-value of 0.0019.\nInterpretation: Both the t-test and the linear regression show a statistically significant difference between the treatment group (those who received the matching grant) and the control group (those who did not). The low p-value (0.0019) indicates that we can reject the null hypothesis and conclude that the treatment had an effect on the likelihood of making a donation.\nThis means that individuals who were offered a matching grant were more likely to make a charitable contribution compared to those who did not receive the offer. In the context of the experiment, this suggests that the presence of a matching gift significantly increases the response rate—the proportion of people who donate. This finding is consistent with the idea that people are more inclined to donate when they perceive their contribution as “more valuable” due to the match, which could be seen as an incentive or psychological nudge.\nIn human behavior terms, this indicates that matching grants can effectively encourage individuals to engage in charitable giving, supporting the idea that price (or the perception of increased value) plays a critical role in donation decisions.\nAfter that I wan to run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control.\n# Running a probit regression where the outcome variable is whether any charitable donation was made (gave)\n# and the explanatory variable is assignment to treatment or control\n# We will exclude missing data for a clean model\n\nfrom statsmodels.discrete.discrete_model import Probit\n\nX_probit = sm.add_constant(data['treatment'])  # adding constant for intercept\ny_probit = data['gave']\n\n# Probit regression model\nprobit_model = Probit(y_probit, X_probit).fit()\n\n# Extracting the results from the probit model\nprobit_model.summary()\nProbit Regression Results: Coefficient for Treatment: The estimated coefficient for the treatment variable is 0.0868, which means that being assigned to the treatment group (matching grant) increases the likelihood of making a charitable donation.\nP-value for Treatment: The p-value is 0.002, which is statistically significant at the 95% confidence level. This suggests that the treatment had a significant effect on the probability of making a donation.\nIntercept (const): The intercept is -2.1001, representing the predicted probability of donation for the control group (those who did not receive a matching grant).\nSummary: The results from this probit regression replicate the findings in Table 3, Column 1 from the paper. The coefficient for Treatment (0.0868) and p-value (0.002) are consistent with the expected outcome, indicating that the matching grant treatment significantly increases the likelihood of a donation.\nThis confirms that the treatment (matching gift) was effective in encouraging donations.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nI use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not.\n# Cleaning the 'ratio' column by converting 'Control' to 0\ndata['ratio'] = data['ratio'].replace('Control', 0)\n\n# Now grouping data by the different match ratios and conducting the t-tests again\n# Extracting the data for the 1:1, 2:1, and 3:1 ratios\nratio_0 = data[data['ratio'] == 0]['gave']  # Control group (no match)\nratio_1 = data[data['ratio'] == 1]['gave']  # 1:1 match group\nratio_2 = data[data['ratio'] == 2]['gave']  # 2:1 match group\nratio_3 = data[data['ratio'] == 3]['gave']  # 3:1 match group\n\n# Conducting t-tests\nt_stat_2_1_vs_1_1, p_value_2_1_vs_1_1 = stats.ttest_ind(ratio_1, ratio_2)  # 2:1 vs 1:1\nt_stat_3_1_vs_1_1, p_value_3_1_vs_1_1 = stats.ttest_ind(ratio_1, ratio_3)  # 3:1 vs 1:1\n\n# Results\nt_stat_2_1_vs_1_1, p_value_2_1_vs_1_1, t_stat_3_1_vs_1_1, p_value_3_1_vs_1_1\nT-test Results: 2:1 vs 1:1 match ratio:\nT-statistic: -0.97\nP-value: 0.33\n3:1 vs 1:1 match ratio:\nT-statistic: -1.02\nP-value: 0.31\nInterpretation: The p-values for both comparisons (2:1 vs 1:1 and 3:1 vs 1:1) are above the 0.05 threshold, indicating no significant difference in the likelihood of making a donation between the different match ratios. Specifically:\nThe 2:1 match ratio does not lead to a significantly higher likelihood of donation compared to the 1:1 ratio.\nSimilarly, the 3:1 match ratio does not show a significant increase in the likelihood of donation when compared to the 1:1 ratio.\nComparison to the Authors’ Comment: The authors’ comment on page 8 suggests that larger match ratios (like 2:1 and 3:1) might increase the likelihood of donation compared to a 1:1 ratio. However, the results from these t-tests do not support that claim. Based on the statistical analysis, there is no evidence to suggest that increasing the match ratio has a significant effect on donation behavior. The results indicate that the match ratio does not appear to significantly influence the decision to donate\nI also assess the same issue using a regression. Specifically, I create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio).\n# Create the categorical variable 'ratio1' based on match ratios (1:1, 2:1, 3:1)\ndata['ratio1'] = data['ratio'].astype('category')\n\n# Run a regression where the outcome variable is 'gave' (whether a donation was made)\n# and the explanatory variables are the match ratios (categorical 'ratio1')\n\n# Ensure that the categorical variable 'ratio1' is properly encoded as numeric\nX_regression = sm.add_constant(pd.get_dummies(data['ratio1'], drop_first=True).astype(float))  # Convert to float for compatibility\ny_regression = data['gave'].astype(float)  # Ensure the dependent variable is numeric\n\n# Run the linear regression model\nregression_model = sm.OLS(y_regression, X_regression, missing='drop').fit()\n\n# Show the summary of the regression results\nregression_model.summary()\nRegression Results Interpretation: Intercept (const): The baseline probability of making a donation for the control group (those who received no matching grant) is 0.0179 (or about 1.79% of the control group donated).\n1 (Coefficient for 1:1 match): The coefficient for the 1:1 match ratio is 0.0029, suggesting a 0.29% increase in the likelihood of making a donation compared to the control group. However, the p-value (0.097) is marginally significant at the 10% level, but not significant at the 5% level.\n2 (Coefficient for 2:1 match): The coefficient for the 2:1 match ratio is 0.0048, showing a 0.48% increase in the likelihood of making a donation compared to the control group. This is statistically significant with a p-value of 0.006, indicating significance at the 1% level.\n3 (Coefficient for 3:1 match): The coefficient for the 3:1 match ratio is 0.0049, showing a 0.49% increase in the likelihood of making a donation compared to the control group. This is statistically significant with a p-value of 0.005, indicating significance at the 1% level.\nStatistical Significance: The coefficients for the 2:1 and 3:1 match ratios are statistically significant at the 1% level, while the coefficient for the 1:1 match is only marginally significant at the 10% level.\nInterpretation of Results: The regression results suggest that larger match ratios (2:1 and 3:1) lead to a statistically significant increase in the likelihood of making a donation, compared to the control group. The 1:1 ratio, while still showing an increase in donations, does not have as strong of an effect, as indicated by the marginally significant p-value.\nNext, I calculated the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios.\n# Extracting the coefficients for the match ratios\ncoef_2_1_vs_1_1 = regression_model.params[1]  # Coefficient for 2:1 match\ncoef_3_1_vs_2_1 = regression_model.params[2]  # Coefficient for 3:1 match\n\n# Calculate the differences in the coefficients\ncoef_diff_2_1_vs_1_1 = coef_2_1_vs_1_1  # 2:1 vs 1:1\ncoef_diff_3_1_vs_2_1 = coef_3_1_vs_2_1 - coef_2_1_vs_1_1  # 3:1 vs 2:1\n\n# Results\ncoef_diff_2_1_vs_1_1, coef_diff_3_1_vs_2_1\nCoefficient Differences: Difference between 2:1 and 1:1 match ratio: The coefficient difference is 0.0029, which indicates a 0.29% increase in the likelihood of donation for the 2:1 match ratio compared to the 1:1 match ratio.\nDifference between 3:1 and 2:1 match ratio: The coefficient difference is 0.0019, indicating a 0.19% increase in the likelihood of donation for the 3:1 match ratio compared to the 2:1 match ratio.\nInterpretation: The results suggest that both the 2:1 and 3:1 match ratios increase the likelihood of donation compared to the 1:1 ratio, but the increase is small.\nThe effect of moving from a 2:1 to a 3:1 ratio is relatively smaller (0.19% increase), indicating diminishing returns as the match ratio increases.\nThese findings suggest that while larger match ratios do have a positive effect on donation behavior, the increase becomes less pronounced as the ratio continues to grow. Therefore, higher match ratios may still have a positive effect, but the effectiveness diminishes at higher levels.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nI first calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status.\n# T-test: Compare the donation amounts between the treatment and control groups\ntreatment_amount = data[data['treatment'] == 1]['amount'].dropna()\ncontrol_amount = data[data['treatment'] == 0]['amount'].dropna()\n\n# Conducting the t-test between treatment and control groups for donation amounts\nt_stat_amount, p_value_amount = stats.ttest_ind(treatment_amount, control_amount)\n\n# Bivariate Linear Regression: Regress donation amount on treatment status\nX_regression_amount = sm.add_constant(data['treatment'])  # Add constant for intercept\ny_regression_amount = data['amount']\n\n# Run the linear regression model\nregression_model_amount = sm.OLS(y_regression_amount, X_regression_amount, missing='drop').fit()\n\n# Results\nt_stat_amount, p_value_amount, regression_model_amount.summary()\nT-test Results: T-statistic: 1.86\nP-value: 0.0628\nThe p-value is just above the 0.05 significance level, indicating that there is a marginally significant difference in the donation amounts between the treatment (matching grant) and control groups. This suggests that the matching grant might slightly increase the donation amount, but the evidence is not strong enough to confidently conclude a significant effect at the 5% level.\nBivariate Linear Regression Results: Intercept (const): The average donation amount for the control group (those who did not receive a matching grant) is $0.8133.\nTreatment Coefficient: The coefficient for the treatment variable is 0.1536, meaning that, on average, individuals in the treatment group donated $0.1536 more than those in the control group. The p-value for this coefficient is 0.063, which is marginally significant at the 10% level but not at the 5% level.\nInterpretation: Both the t-test and the regression suggest that matching grants have a small, marginal effect on donation amounts. Individuals in the treatment group tend to donate more than those in the control group, but the effect is small and only marginally significant.\nThis implies that while matching grants may encourage slightly higher donation amounts, the increase is not large enough to be considered a substantial driver of larger contributions.\nNow I limit the data to just people who made a donation and repeat the previous analysis. This regression allows me to analyze how much respondents donate conditional on donating some positive amount.\n# Filter the data to include only people who made a donation (gave == 1)\ndonors_data = data[data['gave'] == 1]\n\n# T-test: Compare the donation amounts between the treatment and control groups among those who donated\ntreatment_amount_donors = donors_data[donors_data['treatment'] == 1]['amount'].dropna()\ncontrol_amount_donors = donors_data[donors_data['treatment'] == 0]['amount'].dropna()\n\n# Conducting the t-test between treatment and control groups for donation amounts (only for donors)\nt_stat_amount_donors, p_value_amount_donors = stats.ttest_ind(treatment_amount_donors, control_amount_donors)\n\n# Bivariate Linear Regression: Regress donation amount on treatment status among those who donated\nX_regression_amount_donors = sm.add_constant(donors_data['treatment'])  # Add constant for intercept\ny_regression_amount_donors = donors_data['amount']\n\n# Run the linear regression model\nregression_model_amount_donors = sm.OLS(y_regression_amount_donors, X_regression_amount_donors, missing='drop').fit()\n\n# Results\nt_stat_amount_donors, p_value_amount_donors, regression_model_amount_donors.summary()\nT-test Results: T-statistic: -0.58\nP-value: 0.56\nThe p-value is quite high, suggesting that there is no significant difference in the donation amounts between the treatment and control groups among those who donated. This indicates that the matching grant treatment does not significantly affect the donation amounts, conditional on the person deciding to donate.\nBivariate Linear Regression Results: Intercept (const): The estimated average donation amount for individuals in the control group (who did not receive a matching grant) is $45.54.\nTreatment Coefficient: The coefficient for the treatment variable is -1.6684, which suggests that the treatment group, on average, donates $1.67 less than the control group, conditional on donating. However, the p-value for this coefficient is 0.561, indicating that this result is not statistically significant.\nInterpretation: The t-test and regression show that there is no significant effect of receiving the matching grant on the amount donated among those who chose to donate. Although the regression suggests a small negative difference, it is not statistically significant.\nThe treatment coefficient (whether a person was assigned to the matching grant) does not have a strong causal interpretation in this case. The lack of statistical significance means we cannot confidently say that the matching grant causally affects the donation amounts among those who donate. Additionally, there may be other factors influencing donation amounts that are not captured in the model, such as individual donor preferences, income, or other unobserved characteristics.\nIn summary, the matching grant treatment does not appear to have a significant effect on how much people donate once they decide to contribute.\nI then made two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. I added a red vertical bar or some other annotation to indicate the sample average for each plot.\nimport matplotlib.pyplot as plt\n\n# Filter the data to include only people who donated\ntreatment_donors = donors_data[donors_data['treatment'] == 1]['amount']\ncontrol_donors = donors_data[donors_data['treatment'] == 0]['amount']\n\n# Calculate the sample averages for each group\ntreatment_avg = treatment_donors.mean()\ncontrol_avg = control_donors.mean()\n\n# Plot for the Treatment Group\nplt.figure(figsize=(12, 6))\n\n# Treatment Group Histogram\nplt.subplot(1, 2, 1)\nplt.hist(treatment_donors, bins=20, color='blue', edgecolor='black', alpha=0.7)\nplt.axvline(treatment_avg, color='red', linestyle='dashed', linewidth=2)\nplt.title('Treatment Group - Donation Amounts')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.legend(['Sample Average', 'Donation Amounts'])\n\n# Control Group Histogram\nplt.subplot(1, 2, 2)\nplt.hist(control_donors, bins=20, color='orange', edgecolor='black', alpha=0.7)\nplt.axvline(control_avg, color='red', linestyle='dashed', linewidth=2)\nplt.title('Control Group - Donation Amounts')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.legend(['Sample Average', 'Donation Amounts'])\n\nplt.tight_layout()\nplt.show()\n\n\n\nControl Group - Donation Amounts"
  },
  {
    "objectID": "blog/HW 1/index.html#simulation-experiment",
    "href": "blog/HW 1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nI made a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, I simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. I then calculate a vector of 10,000 differences, and then plot the cumulative average of that vector of differences.\nfrom scipy.stats import t\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define t-statistic and degrees of freedom\nt_stat = -2.6\ndf = 100  # degrees of freedom\n\n# Define x values for the t-distribution\nx = np.linspace(-4, 4, 1000)\n\n# Calculate the t-distribution pdf\npdf = t.pdf(x, df)\n\n# Calculate the p-value corresponding to the t-statistic (one-tailed)\np_value = t.cdf(t_stat, df)\n\n# Plot the t-distribution\nplt.plot(x, pdf, color='black', label='t-distribution')\n\n# Highlight the area in the left tail beyond the t-statistic\nx_fill = np.linspace(t_stat, -4, 1000)\ny_fill = t.pdf(x_fill, df)\nplt.fill_between(x_fill, y_fill, color='green', alpha=0.3, label=f'Area: p-value = {p_value:.4f}')\n\n# Mark the t-statistic on the plot\nplt.axvline(x=t_stat, color='red', linestyle='dashed', label=f't-stat = {t_stat}')\n\n# Add labels and title\nplt.title('t-Statistic and Corresponding p-value')\nplt.xlabel('t-value')\nplt.ylabel('Density')\nplt.legend()\n\nplt.grid(True)\nplt.show()\n\n\n\nt-Statistic and Corresponding p-value\n\n\nKey Elements: Black Curve: The t-distribution with the appropriate degrees of freedom.\nRed Dashed Line: The t-statistic of -2.6.\nGreen Shaded Area: The area in the tail of the distribution, corresponding to the p-value. In this case, the p-value is calculated as the area beyond the t-statistic in the left tail of the distribution, which is 0.0054.\nExplanation: The p-value represents the probability of obtaining a result at least as extreme as the observed result, assuming the null hypothesis is true. In this case, the p-value of 0.0054 indicates that we are in the tail of the distribution, which gives us some evidence against the null hypothesis (a result is statistically significant if the p-value is less than 0.05).\n\n\nCentral Limit Theorem\nI make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000.\n# Reimporting the necessary libraries and re-running the simulation to generate the histograms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function to simulate the differences in means for given sample sizes\ndef simulate_diff_in_means(sample_size, control_prob=0.018, treatment_prob=0.022, n_iterations=1000):\n    diff_in_means = []  # List to store the differences in means\n    \n    for _ in range(n_iterations):\n        # Generate random samples from control and treatment distributions\n        control_sample = np.random.binomial(1, control_prob, sample_size)\n        treatment_sample = np.random.binomial(1, treatment_prob, sample_size)\n        \n        # Calculate the difference in means\n        control_mean = np.mean(control_sample)\n        treatment_mean = np.mean(treatment_sample)\n        diff_in_means.append(treatment_mean - control_mean)\n    \n    return diff_in_means\n\n# Simulate the differences in means for sample sizes 50, 200, 500, and 1000\nsample_sizes = [50, 200, 500, 1000]\ndiffs = {}\n\nfor size in sample_sizes:\n    diffs[size] = simulate_diff_in_means(size)\n\n# Plot the histograms for each sample size\nplt.figure(figsize=(16, 10))\n\nfor i, size in enumerate(sample_sizes, 1):\n    plt.subplot(2, 2, i)\n    plt.hist(diffs[size], bins=30, color='blue', edgecolor='black', alpha=0.7)\n    plt.axvline(0, color='red', linestyle='dashed', linewidth=2)\n    plt.title(f'Sample Size = {size}')\n    plt.xlabel('Difference in Means')\n    plt.ylabel('Frequency')\n    plt.tight_layout()\n\nplt.show()\n Key Elements:\nX-axis: Difference in means between the treatment and control groups (calculated for each iteration).\nY-axis: Frequency of occurrence for each difference in means.\nExplanation: Sample Size = 50: The histogram is very wide and unstable, with values spreading across a large range. The cumulative average is less reliable due to the small sample size.\nSample Size = 200: The distribution starts to narrow, with the differences clustering around 0. The estimate of the difference in means is becoming more stable, but there is still some variability.\nSample Size = 500: The distribution becomes much narrower and starts to resemble a normal distribution. The estimates are now more consistent with the true value.\nSample Size = 1000: At this larger sample size, the histogram is quite tight, and the differences in means are centered very closely around the true value of 0.004. The Central Limit Theorem is clearly visible here.\nConclusion: Central Limit Theorem: As the sample size increases, the distribution of the differences in means becomes more concentrated around the true difference (0.004), demonstrating that the estimate becomes more accurate with larger sample sizes. With a sample size of 1000, the distribution is almost normal, and the cumulative average approaches the true value."
  }
]